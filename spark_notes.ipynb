{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d0b5f21-f590-4996-b19b-841c583202bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# About\n",
    "\n",
    "This is a notebook about how to use pyspark.  \n",
    "This is written based on Microsoft Azure Databricks, no guarantees provided for other cloud providers or environments.  \n",
    "Core documentation: https://spark.apache.org/docs/latest/api/python/reference/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fd689c3-6ff3-4277-99ed-6e6535f81e8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SparkSession Object\n",
    "\n",
    "Databricks instantiates a SparkSesion object by default, named `spark`.  \n",
    "In vanilla python, you will need to instantiate it yourself first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebb7a2a7-581d-4ae7-99a4-908f0f8c92ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# â€‹https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d04deb2-c75d-4526-a2a5-623a38a5e9b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Querying SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c8ea8e5-6136-4c46-8f9a-024b14126988",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic SQL Query Methods\n",
    "\n",
    "There are three main ways to query SQL tables with pyspark.\n",
    "\n",
    "1. `spark.sql(query_string)`\n",
    "2. `spark.table(table_name)`\n",
    "3. `spark.read.table(table_name)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd9e421d-9499-4848-aea3-b53df2029b9c",
     "showTitle": false,
     "title": "spark.sql()"
    }
   },
   "source": [
    "### 1. `spark.sql(query_string: str, [**kwargs]) -> pyspark.sql.DataFrame`\n",
    "\n",
    "**Call Tree:**  \n",
    "1. `pyspark.sql.SparkSession` object\n",
    "2. `pyspark.sql.SparkSession.sql()` method\n",
    "3. `pyspark.sql.DataFrame` object`\n",
    "\n",
    "**Docs:**\n",
    "- [SparkSession object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html)\n",
    "- [sql() method](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.sql.html)\n",
    "- [DataFrame object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57fd60ff-4d09-4ee3-9c9f-030b300a3e78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from curated_udp_db.txn_policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5defece1-1a2d-4fe6-8f45-bef5fcdf171e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Notes:\n",
    "\n",
    "##### Programmatic Query via `spark.sql`\n",
    "\n",
    "It is possible to programmatically alter sql queries by providing _kwargs_ options to `spark.sql`.\n",
    "[This is the officially documented method in pyspark.](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.sql.html#pyspark.sql.SparkSession.sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5b2f536-0971-48c4-b4b4-00082b4fb2ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\", bound1=7, bound2=9\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6be5c33d-7506-48cb-9268-df07ca9d036b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Alternatively, using python string `str.format()` method and its variations should have the same behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fc7a3d9-0ce9-416c-8508-596dddb56291",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\".format(bound1=7, bound2=9)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "708a90a1-d8d1-4421-9f3f-e1db16ea474a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. `spark.table(table_name: str) -> pyspark.sql.DataFrame`\n",
    "\n",
    "**Call Tree:**  \n",
    "1. `pyspark.sql.SparkSession` object\n",
    "2. `pyspark.sql.SparkSession.table()` method\n",
    "3. `pyspark.sql.DataFrame` object\n",
    "\n",
    "**Docs:**  \n",
    "- [SparkSession object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html)\n",
    "- [table() method](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.table.html)\n",
    "- [DataFrame object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc19d52f-209d-42b0-9c50-d957dceeb30c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"curated_udp_db.txn_policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f459758-699c-4e35-a6c5-136a39218d35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. `spark.read.table(table_name: str) -> pyspark.sql.DataFrame`\n",
    "\n",
    "**Call Tree:**  \n",
    "1. `pyspark.sql.SparkSession` object\n",
    "2. `pyspark.sql.SparkSession.read` property\n",
    "3. `pyspark.sql.DataFrameReader` object\n",
    "4. `pyspark.sql.DataFrameReader.table()` method\n",
    "5. `pyspark.sql.DataFrame` object\n",
    "\n",
    "**Docs:**  \n",
    "- [SparkSession object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html)\n",
    "- [read property](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.read.html)\n",
    "- [DataFrameReader object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.html)\n",
    "- [table() method](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.table.html)\n",
    "- [DataFrame object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "113ba1ac-be10-4e75-99f4-b13578ff209c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.table(\"curated_udp_db.txn_policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0928f417-6e46-4e6b-89b3-3521b4e42ffa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Programmatic Query via `spark.sql`\n",
    "\n",
    "It is possible to programmatically alter sql queries by providing _kwargs_ options to `spark.sql`. This is the officially documented method in pyspark:  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.sql.html#pyspark.sql.SparkSession.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f9f4379-fd24-46b3-91b6-d849aba260bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\", bound1=7, bound2=9\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d9d5cfa-75cd-415a-9f03-1672156dac3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Alternatively, using python string `format()` method should have the same behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76338e2b-c1fe-4432-81dd-9ecadfe0b421",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\".format(bound1=7, bound2=9)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b12c06e0-4aa4-463c-8f34-2a06ee084dec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Select & Joins\n",
    "\n",
    "You do not need to chain pyspark commands into a single line.  \n",
    "It is entirely possible to create variables of each subquery, so that the combined query is more legible.  \n",
    "_Note, however, it is unclear whether chaining vs separate variable declaration has any impact on query performance._\n",
    "\n",
    "**Example:**  \n",
    "\n",
    "**Chained Commands:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca0c202d-e1a9-4837-9536-3ceca0945a0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"ai_dwh.ai_dwh_txn_claims\").join(\n",
    "    spark.table(\"ai_dwh.ai_dwh_txn_policy\"),\n",
    "    on=\"party_ref\",\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    spark.table(\"ai_dwh.ai_dwh_txn_claims_diagnosis\"),\n",
    "    on=\"party_ref\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a06df2ab-000b-4576-91c0-2bb69e77b0c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Separate Variable Declarations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4312eff-dcae-4b57-8571-95616991a073",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "claims = spark.table(\"ai_dwh.ai_dwh_txn_claims\")\n",
    "policy = spark.table(\"ai_dwh.ai_dwh_txn_policy\")\n",
    "diagno = spark.table(\"ai_dwh.ai_dwh_txn_claims_diagnosis\")\n",
    "\n",
    "df = claims.join(\n",
    "    policy,\n",
    "    on=\"party_ref\",\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    diagno,\n",
    "    on=\"party_ref\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de54eb33-65c5-4938-a1de-ab9477d51817",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Multiple Condition Joins\n",
    "\n",
    "Refer to: https://stackoverflow.com/a/34463562/5675094\n",
    "\n",
    "To join with multiple conditions, use a list of join conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4290018c-59c3-4ffc-9e1e-4273b0afb79f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = ...\n",
    "df2 = ...\n",
    "\n",
    "# Unclear what behavior this is (AND / OR?) -- assuming defaults to AND\n",
    "list_conditions = [\n",
    "    (df1.col_a == df2.col_a),\n",
    "    (df1.col_b == df2.col_b)\n",
    "]\n",
    "\n",
    "and_conditions = [\n",
    "    (df1.col_a == df2.col_a) &\n",
    "    (df1.col_b == df2.col_b)\n",
    "]\n",
    "\n",
    "or_conditions = [\n",
    "    (df1.col_a == df2.col_a) |\n",
    "    (df1.col_b == df2.col_b)\n",
    "]\n",
    "\n",
    "df1.join(\n",
    "    df2,\n",
    "    on=list_conditions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41317ec0-5938-494e-8846-6590db35a7ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Filtering by Dates\n",
    "\n",
    "If you have a pyspark.sql.DataFrame `df` with a datetime column `dates`, this is how you can filter for records after an arbitrary date:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eade4e1-635b-4779-aeb2-dc8287cebffc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**You can compare datetime columns to string, but not int**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b5a1506-5f4c-41ad-9dc6-e94cc33402c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"ai_dwh.ai_dwh_txn_claims\")\n",
    "\n",
    "df.filter(claims.claim_dt >= \"2022\") # This will run\n",
    "df.filter(claims.claim_dt >= 2022) # This will raise an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43c5b39f-50ab-4a03-8dbc-24af54e51b46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Pyspark considers \"YEAR\" to be the same as the date \"YEAR-01-01\"**\n",
    "- _This can ruin inclusive / exclusive range setups, so consider them carefully_\n",
    "- It is recommended to explicitly write out the full ISO date \"YYYY-MM-DD\" to prevent confusion\n",
    "\n",
    "Consider the following examples:\n",
    "- `date > \"2022\"` means `date > \"2022-01-01\"`\n",
    "  - All dates **AFTER** `January 1st 2022` are True (Exclusive of Jan 1st 2022)\n",
    "- `date >= \"2022\"` means `date >= \"2022-01-01\"`\n",
    "  - All dates **FROM and Greater Than** `January 1st 2022` are True (Inclusive of Jan 1st 2022)\n",
    "- `\"2022\" > date` means `\"2022-01-01\" > date`\n",
    "  - All dates **BEFORE** `January 1st 2022` are True (Exclusive of Jan 1st 2022)\n",
    "- `\"2022\" >= date` means `\"2022-01-01\" >= date`\n",
    "  - All dates **FROM and Less Than** `January 1st 2022` are True (Inclusive of Jan 1st 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd1240ff-3275-4d50-b0a9-280fabf4de50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "  data = pd.DataFrame(\n",
    "      data = pd.date_range(start=\"1999-12-30\", end=\"2000-01-02\", freq=\"D\"),\n",
    "      columns = [\"date_col\"]\n",
    "    )\n",
    ")\\\n",
    ".withColumn(\"date_col\", F.col(\"date_col\").cast(\"date\") )\\\n",
    ".withColumn(\"date > 2000\" , F.when((F.col(\"date_col\") > \"2000\") , 1).otherwise(None) )\\\n",
    ".withColumn(\"date >= 2000\", F.when((F.col(\"date_col\") >= \"2000\"), 1).otherwise(None) )\\\n",
    ".withColumn(\"date < 2000\" , F.when((F.col(\"date_col\") < \"2000\") , 1).otherwise(None) )\\\n",
    ".withColumn(\"date <= 2000\", F.when((F.col(\"date_col\") <= \"2000\"), 1).otherwise(None) )\\\n",
    "\\\n",
    ".withColumn(\"2000 > date\" , F.when((\"2000\" > F.col(\"date_col\")) , 1).otherwise(None) )\\\n",
    ".withColumn(\"2000 >= date\", F.when((\"2000\" >= F.col(\"date_col\")), 1).otherwise(None) )\\\n",
    ".withColumn(\"2000 < date\" , F.when((\"2000\" < F.col(\"date_col\")) , 1).otherwise(None) )\\\n",
    ".withColumn(\"2000 <= date\", F.when((\"2000\" <= F.col(\"date_col\")), 1).otherwise(None) )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "569900d6-5f5a-4a22-941d-6374374edf8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Output of the above code block:\n",
    "\n",
    "```\n",
    "+----------+-----------+------------+-----------+------------+-----------+------------+-----------+------------+\n",
    "|  date_col|date > 2000|date >= 2000|date < 2000|date <= 2000|2000 > date|2000 >= date|2000 < date|2000 <= date|\n",
    "+----------+-----------+------------+-----------+------------+-----------+------------+-----------+------------+\n",
    "|1999-12-30|       null|        null|          1|           1|          1|           1|       null|        null|\n",
    "|1999-12-31|       null|        null|          1|           1|          1|           1|       null|        null|\n",
    "|2000-01-01|       null|           1|       null|           1|       null|           1|       null|           1|\n",
    "|2000-01-02|          1|           1|       null|        null|       null|        null|          1|           1|\n",
    "+----------+-----------+------------+-----------+------------+-----------+------------+-----------+------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40e40744-82cd-464f-b581-64f2260925ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eed19b3-e4fd-4881-a332-da5ffa86e03e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Group By\n",
    "\n",
    "### `pyspark.sql.DataFrame.groupBy(*column_names) -> pyspark.sql.GroupedData`\n",
    "\n",
    "Apply the `groupBy()` method on any `sql.DataFrame` with the specified column or list of columns.  \n",
    "Will return a `GroupedData` DataFrame object.  \n",
    "Need to apply `agg()` method afterwards to collect useable data.\n",
    "\n",
    "**Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fd5f88e-7560-4ad0-8a62-4f7341b7a22f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped_df = df.groupBy(\"column\")\n",
    "grouped_df = df.groupBy([\"column_a\", \"column_b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07394c3f-1c43-48a9-9152-7e0583603c5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pivot Function\n",
    "\n",
    "### `pyspark.sql.GroupedData.pivot(column_name, [*values]) -> pyspark.sql.GroupedData`\n",
    "\n",
    "Apply the `pivot()` method on a `GroupData` object (DataFrame after group by action) to create a pivot table.  \n",
    "Pivots by values in the provided column `column_name`.\n",
    "\n",
    "Optionally provide a list of values to pivot by in `[*values]`.\n",
    "This has large benefits to performance, and is recommended to do.\n",
    "Any other existing values in `column_name` but not provided to `[*values]` will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47d4639b-76cc-4d86-bdde-ff095ed91f62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Pure Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "629a2d66-9c9f-4e00-ac75-28ee32564d85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped_df = df.groupBy(\"column_a\").pivot(\"column_b\")\n",
    "grouped_df = df.groupBy(\"column_a\").pivot(\"column_b\", [val_1, val_2, val_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37dfec60-2cd9-4857-9ef8-3479e02c01e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Example for Counting number of claims by incident dates, separating by benefit types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17a21d7-d9a1-49b0-8b20-a7d56b7ab11b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "benefit_codes = ['DENTAL','IPD','OPD']\n",
    "window = W.partitionBy(\"IPD\").orderBy(\"INCIDENT_DATE\")\n",
    "\n",
    "spark.table(\"ai_dwh.eb_claim_records\")\\\n",
    "  .filter(F.col(\"INCIDENT_DATE\").between(\"2023-01-01\", \"2023-01-31\"))\\\n",
    "  .groupBy(\"INCIDENT_DATE\")\\\n",
    "  .pivot(\"BENEFIT_CODE\", benefit_codes)\\\n",
    "  .agg(F.count(\"POLICY_NO\"))\\\n",
    "  .orderBy(\"INCIDENT_DATE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "827321f7-2f39-4d27-ba31-3ac5e77e5ad6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Aggregation Function\n",
    "\n",
    "### `pyspark.sql.GroupedData.agg(*exprs) -> pyspark.sql.DataFrame`\n",
    "\n",
    "Apply the `agg()` method and add desired [aggregation functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#aggregate-functions) to perform aggregation.  \n",
    "You can provide as many aggregation functions as you like, separated by comma.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a25dd0c6-4ba6-4390-b9a2-487be7d7009f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_agg = df_raw.groupBy(\"column_key\")\\\n",
    "        .agg(\n",
    "            F.count_distinct(\"column_a\").alias(\"column_a_counts\"),\n",
    "            F.sum(\"column_b\").alias(\"column_b_sum\"),\n",
    "            F.first(\"column_c\").alias(\"column_c_first\"),\n",
    "            F.last(\"column_d\").alias(\"column_d_last\"),\n",
    "            ...\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5da682f-65d2-406e-8736-f90099e59cda",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Median / Percentile\n",
    "\n",
    "See: https://stackoverflow.com/a/51933027/5675094\n",
    "\n",
    "Use `pyspark.sql.functions.percentile_approx` function to calculate median or any other percentile value of a column.\n",
    "\n",
    "**Docs:**  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.percentile_approx.html\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29ca062d-2460-4c96-a363-57151b16bdda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as psf\n",
    "\n",
    "df = spark.table(\"ai_dwh.ai_dwh_txn_claims\")\n",
    "\n",
    "# Group by hospital that made the claim\n",
    "grouped = df.groupby(\"hospital_name\")\n",
    "\n",
    "# Calculate Median\n",
    "median = grouped.agg(\n",
    "    psf.percentile_approx(\"claim_amt\", 0.5).alias(\"median\")\n",
    ")\n",
    "\n",
    "# Quantiles (Q1, Q2, Q3)\n",
    "boxplot = grouped.agg(\n",
    "    psf.percentile_approx(\"claim_amt\", [0.25, 0.5, 0.75]).alias(\"quantile\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0203ee06-55c0-4bd5-b109-da5471c8113c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Window Function\n",
    "\n",
    "### `pyspark.sql.window.Window`\n",
    "\n",
    "[The Window utility function](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/window.html) is used to first define a \"window specification\" before you can apply [Window aggregation functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#window-functions); where you have to provide the defined \"window specification\" variable to the window aggregation functions as an argument.\n",
    "\n",
    "The core syntax is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c21ef885-1bab-42c8-8076-47c8cde83462",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define window specification with one of the listed Window methods\n",
    "window_spec = pyspark.sql.window.Window.[window_method()]\n",
    "\n",
    "# Apply window aggregation function and provide defined window specification\n",
    "df.select( pyspark.sql.functions.[window_agg_functions()] )\n",
    "df.withColumn(\"name\", pyspark.sql.functions.[window_agg_functions()] )\n",
    "grouped_df.agg( pyspark.sql.functions.[window_agg_functions()] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d79ae1d-2648-4eeb-9665-f857c4442e77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Example: You want to find the percentile ranking of each value in a DataFrame's column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61bd7c34-ba1e-4742-8ec1-18285e09c1f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.window as W\n",
    "\n",
    "# DataFrame has columns: [user_id: string, number_of_claims: int]\n",
    "\n",
    "# Define window specification to sort DataFrame by the specified column\n",
    "window_spec = W.Window.orderBy(\"number_of_claims\")\n",
    "\n",
    "# Apply Window aggregation function\n",
    "df_rank = df.withColumn(\"percentile_rank\", F.percent_rank().over(window_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f17391b0-1bcd-42f1-bf14-96a6e087a26a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Collecting Ordered List\n",
    "\n",
    "Great answer copied from: https://stackoverflow.com/a/50668635/5675094\n",
    "\n",
    "As you know, using `collect_list` together with `groupBy` will result in an **unordered** list of values. This is because depending on how your data is partitioned, Spark will append values to your list as soon as it finds a row in the group. The order then depends on how Spark plans your aggregation over the executors.\n",
    "\n",
    "A `Window` function allows you to control that situation, grouping rows by a certain value so you can perform an operation `over` each of the resultant groups:\n",
    "\n",
    "```\n",
    "w = Window.partitionBy('id').orderBy('date')\n",
    "```\n",
    "\n",
    "    `partitionBy` - you want groups/partitions of rows with the same id\n",
    "    `orderBy` - you want each row in the group to be sorted by date\n",
    "\n",
    "Once you have defined the scope of your `Window` - \"rows with the same `id`, sorted by `date`\" -, you can use it to perform an operation over it, in this case, a `collect_list`:\n",
    "\n",
    "```\n",
    "F.collect_list('value').over(w)\n",
    "```\n",
    "\n",
    "At this point you created a new column `sorted_list` with an ordered list of values, sorted by date, but you still have duplicated rows per `id`. To trim out the duplicated rows you want to `groupBy` `id` and keep the `max` value in for each group:\n",
    "\n",
    "```\n",
    ".groupBy('id')\\\n",
    ".agg(F.max('sorted_list').alias('sorted_list'))\n",
    "```\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f43c0005-ccaa-4d47-8a76-696adc3228d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window as W\n",
    "\n",
    "window_spec = W.partitionBy('id').orderBy('date')\n",
    "\n",
    "sorted_list_df = input_df.withColumn(\n",
    "            'sorted_list', F.collect_list('value').over(window_spec)\n",
    "        )\\\n",
    "        .groupBy('id')\\\n",
    "        .agg(F.max('sorted_list').alias('sorted_list'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a9c7740-44bc-46ec-8d7c-7ecaa2f6eb94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pyspark UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c39080e6-b822-450b-b1d1-46a24bb22206",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `pyspark.sql.functions.udf`\n",
    "\n",
    "`pyspark.sql.functions.udf` is the preferred pattern:  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html\n",
    "\n",
    "Despite what the documentation says, `pyspark.sql.functions.pandas_udf` is not an alias of `udf` and has different behaviors:  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3efb998d-41a4-4a2f-a36d-a5999b9c77c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html\n",
    "from pyspark.sql import functions as F\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "\n",
    "# Define function in python\n",
    "def py_my_func(x):\n",
    "  # ...\n",
    "  return\n",
    "\n",
    "# Wrap with pyspark udf and specify return type\n",
    "my_func = F.udf(py_my_func, StringType())\n",
    "my_func = F.udf(py_my_func, BooleanType())\n",
    "\n",
    "# Apply the defined function\n",
    "df = group_df.agg(my_func(\"column\"))\n",
    "df = df.withColumn(\"new_column\", my_func(\"column\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "679eeae2-2f56-4914-9550-dcbb837cabe1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "spark_notes",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('3.9.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a688e2845b1413149de257866022b321a66a51890e241f01078c08ce3770621d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
