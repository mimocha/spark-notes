{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2d0b5f21-f590-4996-b19b-841c583202bd","showTitle":false,"title":""}},"source":["# About\n","\n","This is a notebook about how to use pyspark.  \n","This is written based on Microsoft Azure Databricks, no guarantees provided for other cloud providers or environments.  \n","Core documentation: https://spark.apache.org/docs/latest/api/python/reference/index.html"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7fd689c3-6ff3-4277-99ed-6e6535f81e8d","showTitle":false,"title":""}},"source":["# SparkSession Object\n","\n","Databricks instantiates a SparkSesion object by default, named `spark`.  \n","In vanilla python, you will need to instantiate it yourself first."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ebb7a2a7-581d-4ae7-99a4-908f0f8c92ec","showTitle":false,"title":""}},"outputs":[],"source":["# â€‹https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5d04deb2-c75d-4526-a2a5-623a38a5e9b9","showTitle":false,"title":""}},"source":["# Querying SQL"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5c8ea8e5-6136-4c46-8f9a-024b14126988","showTitle":false,"title":""}},"source":["## Basic SQL Query Methods\n","\n","There are three main ways to query SQL tables with pyspark.\n","\n","1. `spark.sql(query_string)`\n","2. `spark.table(table_name)`\n","3. `spark.read.table(table_name)`"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"bd9e421d-9499-4848-aea3-b53df2029b9c","showTitle":false,"title":"spark.sql()"}},"source":["### 1. `spark.sql(query_string: str, [**kwargs]) -> pyspark.sql.DataFrame`\n","\n","**Call Tree:**  \n","1. `pyspark.sql.SparkSession` object\n","2. `pyspark.sql.SparkSession.sql()` method\n","3. `pyspark.sql.DataFrame` object`\n","\n","**Docs:**\n","- [SparkSession object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html)\n","- [sql() method](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.sql.html)\n","- [DataFrame object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)\n","\n","**Example:**  \n","`spark.sql(\"select * from curated_udp_db.txn_policy\")`\n","\n","#### Notes:\n","\n","##### Programmatic Query via `spark.sql`\n","\n","It is possible to programmatically alter sql queries by providing _kwargs_ options to `spark.sql`.\n","[This is the officially documented method in pyspark.](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.sql.html#pyspark.sql.SparkSession.sql)\n","\n","```\n","spark.sql(\n","    \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\", bound1=7, bound2=9\n",").show()\n","```\n","\n","Alternatively, using python string `str.format()` method and its variations should have the same behavior:\n","\n","```\n","spark.sql(\n","    \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\".format(bound1=7, bound2=9)\n",").show()\n","```"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"708a90a1-d8d1-4421-9f3f-e1db16ea474a","showTitle":false,"title":""}},"source":["### 2. `spark.table(table_name: str) -> pyspark.sql.DataFrame`\n","\n","**Call Tree:**  \n","1. `pyspark.sql.SparkSession` object\n","2. `pyspark.sql.SparkSession.table()` method\n","3. `pyspark.sql.DataFrame` object\n","\n","**Docs:**  \n","- [SparkSession object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html)\n","- [table() method](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.table.html)\n","- [DataFrame object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)\n","\n","**Example:**  \n","`spark.table(\"curated_udp_db.txn_policy\")`"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9f459758-699c-4e35-a6c5-136a39218d35","showTitle":false,"title":""}},"source":["### 3. `spark.read.table(table_name: str) -> pyspark.sql.DataFrame`\n","\n","**Call Tree:**  \n","1. `pyspark.sql.SparkSession` object\n","2. `pyspark.sql.SparkSession.read` property\n","3. `pyspark.sql.DataFrameReader` object\n","4. `pyspark.sql.DataFrameReader.table()` method\n","5. `pyspark.sql.DataFrame` object\n","\n","**Docs:**  \n","- [SparkSession object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html)\n","- [read property](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.read.html)\n","- [DataFrameReader object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.html)\n","- [table() method](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.table.html)\n","- [DataFrame object](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)\n","\n","**Example:**  \n","`spark.read.table(\"curated_udp_db.txn_policy\")`"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0928f417-6e46-4e6b-89b3-3521b4e42ffa","showTitle":false,"title":""}},"source":["## Programmatic Query via `spark.sql`\n","\n","It is possible to programmatically alter sql queries by providing _kwargs_ options to `spark.sql`. This is the officially documented method in pyspark:  \n","https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.sql.html#pyspark.sql.SparkSession.sql\n","\n","```\n","spark.sql(\n","    \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\", bound1=7, bound2=9\n",").show()\n","```\n","\n","Alternatively, using python string `format()` method should have the same behavior:\n","\n","```\n","spark.sql(\n","    \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\".format(bound1=7, bound2=9)\n",").show()\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b12c06e0-4aa4-463c-8f34-2a06ee084dec","showTitle":false,"title":""}},"source":["## Select & Joins\n","\n","You do not need to chain pyspark commands into a single line.  \n","It is entirely possible to create variables of each subquery, so that the combined query is more legible.  \n","_Note, however, it is unclear whether chaining vs separate variable declaration has any impact on query performance._\n","\n","**Example:**  \n","\n","**Chained Commands:**  \n","```\n","df = spark.table(\"ai_dwh.ai_dwh_txn_claims\").join(\n","    spark.table(\"ai_dwh.ai_dwh_txn_policy\"),\n","    on=\"party_ref\",\n","    how=\"left\"\n",").join(\n","    spark.table(\"ai_dwh.ai_dwh_txn_claims_diagnosis\"),\n","    on=\"party_ref\",\n","    how=\"left\"\n",")\n","```\n","\n","**Separate Variable Declarations:**  \n","```\n","claims = spark.table(\"ai_dwh.ai_dwh_txn_claims\")\n","policy = spark.table(\"ai_dwh.ai_dwh_txn_policy\")\n","diagno = spark.table(\"ai_dwh.ai_dwh_txn_claims_diagnosis\")\n","\n","df = claims.join(\n","    policy,\n","    on=\"party_ref\",\n","    how=\"left\"\n",").join(\n","    diagno,\n","    on=\"party_ref\",\n","    how=\"left\"\n",")\n","```"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"de54eb33-65c5-4938-a1de-ab9477d51817","showTitle":false,"title":""}},"source":["## Multiple Condition Joins\n","\n","Refer to: https://stackoverflow.com/a/34463562/5675094\n","\n","To join with multiple conditions, use a list of join conditions:\n","\n","```python\n","df1 = ...\n","df2 = ...\n","\n","# Unclear what behavior this is (AND / OR?) -- assuming defaults to AND\n","list_conditions = [\n","    (df1.col_a == df2.col_a),\n","    (df1.col_b == df2.col_b)\n","]\n","\n","and_conditions = [\n","    (df1.col_a == df2.col_a) &\n","    (df1.col_b == df2.col_b)\n","]\n","\n","or_conditions = [\n","    (df1.col_a == df2.col_a) |\n","    (df1.col_b == df2.col_b)\n","]\n","\n","df1.join(\n","    df2,\n","    on=list_conditions\n",")\n","```"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"41317ec0-5938-494e-8846-6590db35a7ca","showTitle":false,"title":""}},"source":["## Filtering by Dates\n","\n","If you have a pyspark.sql.DataFrame `df` with a datetime column `dates`,\n","this is how you can filter for records after an arbitrary date:\n","\n","```python\n","df = spark.table(\"ai_dwh.ai_dwh_txn_claims\")\n","```\n","\n","You can compare datetime columns to string, but not int\n","\n","```python\n","df.filter(claims.claim_dt >= \"2022\") # This will run\n","df.filter(claims.claim_dt >= 2022) # This will raise an error\n","```"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"40e40744-82cd-464f-b581-64f2260925ab","showTitle":false,"title":""}},"source":["# Aggregation"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4eed19b3-e4fd-4881-a332-da5ffa86e03e","showTitle":false,"title":""}},"source":["## Group By\n","\n","### `pyspark.sql.DataFrame.groupBy(*column_names) -> pyspark.sql.GroupedData`\n","\n","Apply the `groupBy()` method on any `sql.DataFrame` with the specified column or list of columns.  \n","Will return a `GroupedData` DataFrame object.  \n","Need to apply `agg()` method afterwards to collect useable data.\n","\n","Examples:\n","\n","- `grouped_df = df.groupBy(\"column\")`\n","- `grouped_df = df.groupBy([\"column_a\", \"column_b\"])`"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Pivot Function\n","\n","### `pyspark.sql.GroupedData.pivot(column_name, [*values]) -> pyspark.sql.GroupedData`\n","\n","Apply the `pivot()` method on a `GroupData` object (DataFrame after group by action) to create a pivot table.  \n","Pivots by values in the provided column `column_name`.\n","\n","Optionally provide a list of values to pivot by in `[*values]`.\n","This has large benefits to performance, and is recommended to do.\n","Any other existing values in `column_name` but not provided to `[*values]` will be ignored.\n","\n","Examples:\n","- `grouped_df = df.groupBy(\"column_a\").pivot(\"column_b\")`\n","- `grouped_df = df.groupBy(\"column_a\").pivot(\"column_b\", [val_1, val_2, val_3])`"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Aggregation Function\n","\n","### `pyspark.sql.GroupedData.agg(*exprs) -> pyspark.sql.DataFrame`\n","\n","Apply the `agg()` method and add desired [aggregation functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#aggregate-functions) to perform aggregation.  \n","You can provide as many aggregation functions as you like, separated by comma.\n","\n","Example:\n","\n","```python\n","import pyspark.sql.functions as F\n","\n","df_agg = df_raw.groupBy(\"column_key\")\\\n","        .agg(\n","            F.count_distinct(\"column_a\").alias(\"column_a_counts\"),\n","            F.sum(\"column_b\").alias(\"column_b_sum\"),\n","            F.first(\"column_c\").alias(\"column_c_first\"),\n","            F.last(\"column_d\").alias(\"column_d_last\"),\n","            ...\n","        )\n","```"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b5da682f-65d2-406e-8736-f90099e59cda","showTitle":false,"title":""}},"source":["### Median / Percentile\n","\n","See: https://stackoverflow.com/a/51933027/5675094\n","\n","Use `pyspark.sql.functions.percentile_approx` function to calculate median or any other percentile value of a column.\n","\n","**Docs:**  \n","https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.percentile_approx.html\n","\n","**Example:**  \n","```python\n","from pyspark.sql import functions as psf\n","\n","df = spark.table(\"ai_dwh.ai_dwh_txn_claims\")\n","\n","# Group by hospital that made the claim\n","grouped = df.groupby(\"hospital_name\")\n","\n","# Calculate Median\n","median = grouped.agg(\n","    psf.percentile_approx(\"claim_amt\", 0.5).alias(\"median\")\n",")\n","\n","# Quantiles (Q1, Q2, Q3)\n","boxplot = grouped.agg(\n","    psf.percentile_approx(\"claim_amt\", [0.25, 0.5, 0.75]).alias(\"quantile\")\n",")\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Window Function\n","\n","### `pyspark.sql.window.Window`\n","\n","[The Window utility function](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/window.html) is used to first define a \"window specification\" before you can apply [Window aggregation functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#window-functions); where you have to provide the defined \"window specification\" variable to the window aggregation functions as an argument.\n","\n","The core syntax is:\n","\n","```python\n","# Define window specification with one of the listed Window methods\n","window_spec = pyspark.sql.window.Window.[window_method()]\n","\n","# Apply window aggregation function and provide defined window specification\n","df.select( pyspark.sql.functions.[window_agg_functions()] )\n","df.withColumn(\"name\", pyspark.sql.functions.[window_agg_functions()] )\n","grouped_df.agg( pyspark.sql.functions.[window_agg_functions()] )\n","```\n","\n","Example: You want to find the percentile ranking of each value in a DataFrame's column.\n","\n","```python\n","import pyspark.sql.functions as F\n","import pyspark.sql.window as W\n","\n","# DataFrame has columns: [user_id: string, number_of_claims: int]\n","\n","# Define window specification to sort DataFrame by the specified column\n","window_spec = W.Window.orderBy(\"number_of_claims\")\n","\n","# Apply Window aggregation function\n","df_rank = df.withColumn(\"percentile_rank\", F.percent_rank().over(window_spec))\n","\n","```"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4a9c7740-44bc-46ec-8d7c-7ecaa2f6eb94","showTitle":false,"title":""}},"source":["# Pyspark UDF"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c39080e6-b822-450b-b1d1-46a24bb22206","showTitle":false,"title":""}},"source":["## `pyspark.sql.functions.udf`\n","\n","`pyspark.sql.functions.udf` is the preferred pattern:  \n","https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html\n","\n","Despite what the documentation says, `pyspark.sql.functions.pandas_udf` is not an alias of `udf` and has different behaviors:  \n","https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html\n","\n","```python\n","# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html\n","from pyspark.sql.functions import udf\n","# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html\n","from pyspark.sql.types import StringType as MY_TYPE\n","\n","# Define function in python\n","def py_my_func(x):\n","  # ...\n","  return\n","\n","# Wrap with pyspark udf\n","my_func = udf(\n","  lambda x: py_my_func(x), # Your defined python function\n","  MY_TYPE # Pyspark output type\n",")\n","\n","# Apply function\n","df = group_df.agg(\n","  my_func(\"column\")\n",")\n","```"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"spark_notes","notebookOrigID":4258470338218956,"widgets":{}},"kernelspec":{"display_name":"Python 3.9.5 64-bit ('3.9.5')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.5"},"vscode":{"interpreter":{"hash":"a688e2845b1413149de257866022b321a66a51890e241f01078c08ce3770621d"}}},"nbformat":4,"nbformat_minor":0}
